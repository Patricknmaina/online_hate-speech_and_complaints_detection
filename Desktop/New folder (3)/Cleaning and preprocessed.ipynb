{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import Union, List, Optional, Dict, Any\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning.ipynb\n",
      "Safaricom tweets.csv\n",
      "online_hate-speech_and_complaints_detection\n",
      "saf_tweets_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "#checking files present\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Replies</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>Views</th>\n",
       "      <th>Date</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.950000e+18</td>\n",
       "      <td>https://x.com/MawiaDorothy/status/194955836816...</td>\n",
       "      <td>How comes I have overdue debts.. na sijakopa.....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>July 27, 2025 at 07:51 PM</td>\n",
       "      <td>Customer care complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.950000e+18</td>\n",
       "      <td>https://x.com/KruiGeofrey/status/1949310365839...</td>\n",
       "      <td>@Monty_Hasashi @Safaricom ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>July 27, 2025 at 03:26 AM</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.950000e+18</td>\n",
       "      <td>https://x.com/martozgicha/status/1949022872242...</td>\n",
       "      <td>@safaricom weka data ,wacheni jokes...Thank yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>July 26, 2025 at 08:23 AM</td>\n",
       "      <td>Internet or airtime bundle complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.950000e+18</td>\n",
       "      <td>https://x.com/liyansmutembei/status/1948476756...</td>\n",
       "      <td>@SafaricomPLC Hello @SafaricomPLC   @safaricom...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>July 24, 2025 at 08:13 PM</td>\n",
       "      <td>Customer care complaint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.950000e+18</td>\n",
       "      <td>https://x.com/SsirNixoNdugire/status/194833516...</td>\n",
       "      <td>@PeterNdegwa_ @SafaricomPLC @Safaricom_Care @S...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>July 24, 2025 at 10:51 AM</td>\n",
       "      <td>Customer care complaint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet ID                                                URL  \\\n",
       "0  1.950000e+18  https://x.com/MawiaDorothy/status/194955836816...   \n",
       "1  1.950000e+18  https://x.com/KruiGeofrey/status/1949310365839...   \n",
       "2  1.950000e+18  https://x.com/martozgicha/status/1949022872242...   \n",
       "3  1.950000e+18  https://x.com/liyansmutembei/status/1948476756...   \n",
       "4  1.950000e+18  https://x.com/SsirNixoNdugire/status/194833516...   \n",
       "\n",
       "                                             Content  Likes  Retweets  \\\n",
       "0  How comes I have overdue debts.. na sijakopa.....      1         0   \n",
       "1                       @Monty_Hasashi @Safaricom ðŸ˜‚ðŸ˜‚      0         0   \n",
       "2  @safaricom weka data ,wacheni jokes...Thank yo...      0         0   \n",
       "3  @SafaricomPLC Hello @SafaricomPLC   @safaricom...      0         0   \n",
       "4  @PeterNdegwa_ @SafaricomPLC @Safaricom_Care @S...      0         0   \n",
       "\n",
       "   Replies  Quotes  Views                       Date  \\\n",
       "0        0       0     21  July 27, 2025 at 07:51 PM   \n",
       "1        0       0     22  July 27, 2025 at 03:26 AM   \n",
       "2        0       0      6  July 26, 2025 at 08:23 AM   \n",
       "3        0       0     47  July 24, 2025 at 08:13 PM   \n",
       "4        0       0      5  July 24, 2025 at 10:51 AM   \n",
       "\n",
       "                                 Labels  \n",
       "0               Customer care complaint  \n",
       "1                               Neutral  \n",
       "2  Internet or airtime bundle complaint  \n",
       "3               Customer care complaint  \n",
       "4               Customer care complaint  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Safaricom tweets dataset\n",
    "Saf_tweets=pd.read_csv('Safaricom tweets.csv')\n",
    "#Displaying the first few rows of the dataset\n",
    "Saf_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2574 entries, 0 to 2573\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Tweet ID  2574 non-null   float64\n",
      " 1   URL       2574 non-null   object \n",
      " 2   Content   2574 non-null   object \n",
      " 3   Likes     2574 non-null   int64  \n",
      " 4   Retweets  2574 non-null   int64  \n",
      " 5   Replies   2574 non-null   int64  \n",
      " 6   Quotes    2574 non-null   int64  \n",
      " 7   Views     2574 non-null   int64  \n",
      " 8   Date      2574 non-null   object \n",
      " 9   Labels    2573 non-null   object \n",
      "dtypes: float64(1), int64(5), object(4)\n",
      "memory usage: 201.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#checking info\n",
    "Saf_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral                                 1032\n",
       "Customer care complaint                  397\n",
       "Internet or airtime bundle complaint     299\n",
       "Hate Speech                              297\n",
       "MPESA complaint                          189\n",
       "Network reliability problem              184\n",
       "Data protection and privacy concern      175\n",
       "Name: Labels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check unique counts in labels\n",
    "Saf_tweets['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for duplicated values\n",
    "Saf_tweets.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contradiction dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "    \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
    "    \"'d\": \" would\", \"'m\": \" am\", \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\", \"what's\": \"what is\",\n",
    "    \"there's\": \"there is\", \"here's\": \"here is\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_text(text, contractions=contractions):\n",
    "    \"\"\"Expand contractions in the text.\"\"\"\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(text):\n",
    "    \"\"\"Reduce repeated characters (e.g., soooo â†’ soo).\"\"\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(text, \n",
    "                   remove_urls=True,\n",
    "                   remove_mentions=True,\n",
    "                   remove_hashtags=True):\n",
    "    \"\"\"Apply basic regex cleaning to text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    if remove_mentions:\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    if remove_hashtags:\n",
    "        text = re.sub(r'#', '', text)\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?]', '', text)  # Remove special chars but keep ! ?\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\?+', '?', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_pipeline(text,\n",
    "                        expand_contractions=True,\n",
    "                        remove_repeated_chars=True,\n",
    "                        remove_urls=True,\n",
    "                        remove_mentions=True,\n",
    "                        remove_hashtags=True):\n",
    "    \"\"\"Complete cleaning pipeline (no tokenizing or lemmatizing).\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "\n",
    "    if expand_contractions:\n",
    "        text = expand_contractions_text(text)\n",
    "    \n",
    "    if remove_repeated_chars:\n",
    "        text = remove_repeated_characters(text)\n",
    "\n",
    "    text = basic_cleaning(\n",
    "        text,\n",
    "        remove_urls=remove_urls,\n",
    "        remove_mentions=remove_mentions,\n",
    "        remove_hashtags=remove_hashtags\n",
    "    )\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if it has worked with some tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: My network is misbehaving\n",
      "Cleaned: rudisheni hii na mnipee bundles sasa sms nazifanyia nini\n",
      "Cleaned: \n",
      "Cleaned: you are a scam\n"
     ]
    }
   ],
   "source": [
    "tweets = [\n",
    "    \"My @safaricom network is misbehaving\",\n",
    "    \"@safaricom rudisheni hii na mnipee bundles .sasa sms nazifanyia nini https://t.co/CvaD1kd5wM\",\n",
    "    \"@Shikanda_00 @safaricom\",\n",
    "    \"@safaricom you are a scam https://t.co/80BRkJ5uB2\"\n",
    "]\n",
    "\n",
    "for t in tweets:\n",
    "    print(\"Cleaned:\", clean_text_pipeline(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing it on the Safaricom tweets dataset\n",
    "Saf_tweets['Cleaned_Text'] = Saf_tweets['Content'].apply(clean_text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How comes I have overdue debts.. na sijakopa.....</td>\n",
       "      <td>How comes I have overdue debts na sijakopawhat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Monty_Hasashi @Safaricom ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@safaricom weka data ,wacheni jokes...Thank yo...</td>\n",
       "      <td>weka data wacheni jokesThank you for being par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SafaricomPLC Hello @SafaricomPLC   @safaricom...</td>\n",
       "      <td>Hello can you borrow from Airtel and allow man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@PeterNdegwa_ @SafaricomPLC @Safaricom_Care @S...</td>\n",
       "      <td>Jambo Kindly consider introducing a Narration ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  How comes I have overdue debts.. na sijakopa.....   \n",
       "1                       @Monty_Hasashi @Safaricom ðŸ˜‚ðŸ˜‚   \n",
       "2  @safaricom weka data ,wacheni jokes...Thank yo...   \n",
       "3  @SafaricomPLC Hello @SafaricomPLC   @safaricom...   \n",
       "4  @PeterNdegwa_ @SafaricomPLC @Safaricom_Care @S...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0  How comes I have overdue debts na sijakopawhat...  \n",
       "1                                                     \n",
       "2  weka data wacheni jokesThank you for being par...  \n",
       "3  Hello can you borrow from Airtel and allow man...  \n",
       "4  Jambo Kindly consider introducing a Narration ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Saf_tweets[['Content', 'Cleaned_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saf_tweets[['Content', 'Cleaned_Text']].to_csv('saf_tweets_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning.ipynb\n",
      "Safaricom tweets.csv\n",
      "online_hate-speech_and_complaints_detection\n",
      "saf_tweets_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced TweetPreprocessor created with advanced features:\n"
     ]
    }
   ],
   "source": [
    "class EnhancedTextCleaner:\n",
    "    \"\"\"A preprocessing class for Safaricom tweets analysis.\n",
    "    \n",
    "    This class handles:\n",
    "    - Data cleaning (removing URLs, mentions, hashtags, special characters)\n",
    "    - Text preprocessing (tokenization, lemmatization, stop words removal)\n",
    "    - Feature extraction using TF-IDF or Count Vectorization\n",
    "\n",
    "    This class can be implemented directly in a scikit-learn pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lemmatizer=None, \n",
    "                 stop_words=None, \n",
    "                 min_length=2,\n",
    "                 max_length=50,\n",
    "                 remove_numbers=True,\n",
    "                 custom_patterns=None):\n",
    "        self.lemmatizer = lemmatizer or WordNetLemmatizer()\n",
    "        self.stop_words = stop_words or set(stopwords.words('english'))\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.custom_patterns = custom_patterns or []\n",
    "        \n",
    "    def clean_and_lemmatize(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Clean tokens with enhanced filtering and lemmatization.\"\"\"\n",
    "        if not tokens:\n",
    "            return []\n",
    "            \n",
    "        # Filter tokens\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            # Length check\n",
    "            if not (self.min_length <= len(token) <= self.max_length):\n",
    "                continue\n",
    "            # Stop words check\n",
    "            if token.lower() in self.stop_words:\n",
    "                continue\n",
    "            # Numbers check\n",
    "            if self.remove_numbers and token.isdigit():\n",
    "                continue\n",
    "            # Custom patterns check\n",
    "            if any(re.search(pattern, token) for pattern in self.custom_patterns):\n",
    "                continue\n",
    "                \n",
    "            filtered.append(token)\n",
    "        \n",
    "        # Lemmatize\n",
    "        return [self.lemmatizer.lemmatize(token.lower()) for token in filtered]\n",
    "\n",
    "class EnhancedTweetPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Enhanced tweet preprocessor with advanced features:\n",
    "    - Better text cleaning with configurable options\n",
    "    - Support for multiple input formats\n",
    "    - Robust error handling and logging\n",
    "    - Feature extraction statistics\n",
    "    - Memory-efficient processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 text_cleaner: Optional[EnhancedTextCleaner] = None,\n",
    "                 lowercase: bool = True,\n",
    "                 use_tfidf: bool = True,\n",
    "                 vectorizer_params: Optional[Dict[str, Any]] = None,\n",
    "                 preserve_case_words: Optional[List[str]] = None,\n",
    "                 enable_logging: bool = False):\n",
    "        \n",
    "        # Initialize text cleaner\n",
    "        if text_cleaner is None:\n",
    "            try:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                self.text_cleaner = EnhancedTextCleaner(lemmatizer, stop_words)\n",
    "            except LookupError:\n",
    "                # Fallback if NLTK data not available\n",
    "                self.text_cleaner = EnhancedTextCleaner()\n",
    "        else:\n",
    "            self.text_cleaner = text_cleaner\n",
    "            \n",
    "        self.lowercase = lowercase\n",
    "        self.use_tfidf = use_tfidf\n",
    "        self.preserve_case_words = set(preserve_case_words or [])\n",
    "        self.vectorizer = None\n",
    "        self.feature_stats_ = {}\n",
    "        \n",
    "        # Default vectorizer parameters\n",
    "        default_params = {\n",
    "            'max_features': 5000,\n",
    "            'ngram_range': (1, 2),\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.95,\n",
    "            'stop_words': 'english'\n",
    "        }\n",
    "        self.vectorizer_params = {**default_params, **(vectorizer_params or {})}\n",
    "        \n",
    "        # Setup logging\n",
    "        if enable_logging:\n",
    "            logging.basicConfig(level=logging.INFO)\n",
    "            self.logger = logging.getLogger(__name__)\n",
    "        else:\n",
    "            self.logger = None\n",
    "\n",
    "    def _log(self, message: str):\n",
    "        \"\"\"Log message if logging is enabled.\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(message)\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced text cleaning with better pattern matching.\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Preserve case for specific words before lowercasing\n",
    "        preserved_words = {}\n",
    "        for word in self.preserve_case_words:\n",
    "            if word in text:\n",
    "                placeholder = f\"__PRESERVE_{len(preserved_words)}__\"\n",
    "                preserved_words[placeholder] = word\n",
    "                text = text.replace(word, placeholder)\n",
    "        \n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Enhanced cleaning patterns\n",
    "        patterns = [\n",
    "            (r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ''),  # URLs\n",
    "            (r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ''),  # www URLs\n",
    "            (r'@[A-Za-z0-9_]+', ''),  # Mentions\n",
    "            (r'#[A-Za-z0-9_]+', ''),  # Hashtags\n",
    "            (r'RT\\s+', ''),  # Retweet indicators\n",
    "            (r'\\\b\\d+\\\b', ''),  # Numbers\n",
    "            (r'[^\\w\\s]', ' '),  # Non-alphanumeric except spaces\n",
    "            (r'\\s+', ' ')  # Multiple spaces\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in patterns:\n",
    "            text = re.sub(pattern, replacement, text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Restore preserved words\n",
    "        for placeholder, original in preserved_words.items():\n",
    "            text = text.replace(placeholder, original)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text with enhanced tokenization.\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = text.split()\n",
    "        cleaned_tokens = self.text_cleaner.clean_and_lemmatize(tokens)\n",
    "        \n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    def _extract_text_from_input(self, X) -> List[str]:\n",
    "        \"\"\"Extract text from various input formats with better error handling.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Try common text column names\n",
    "            text_columns = ['text', 'content', 'tweet', 'message', 'Content', 'Cleaned_Text', 'body']\n",
    "            for col in text_columns:\n",
    "                if col in X.columns:\n",
    "                    self._log(f\"Using column '{col}' for text extraction\")\n",
    "                    return X[col].fillna('').astype(str).tolist()\n",
    "            \n",
    "            # Fallback to first column\n",
    "            self._log(\"Using first column for text extraction\")\n",
    "            return X.iloc[:, 0].fillna('').astype(str).tolist()\n",
    "            \n",
    "        elif isinstance(X, pd.Series):\n",
    "            return X.fillna('').astype(str).tolist()\n",
    "        elif isinstance(X, (list, tuple)):\n",
    "            return [str(x) for x in X]\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            return [str(x) for x in X.flatten()]\n",
    "        else:\n",
    "            return [str(X)]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessor with enhanced statistics tracking.\"\"\"\n",
    "        self._log(\"Starting fit process\")\n",
    "        \n",
    "        text_data = self._extract_text_from_input(X)\n",
    "        self._log(f\"Extracted {len(text_data)} text samples\")\n",
    "        \n",
    "        # Preprocess texts\n",
    "        processed_texts = []\n",
    "        empty_count = 0\n",
    "        \n",
    "        for text in text_data:\n",
    "            cleaned = self.clean_text(text)\n",
    "            processed = self.preprocess_text(cleaned)\n",
    "            \n",
    "            if processed.strip():\n",
    "                processed_texts.append(processed)\n",
    "            else:\n",
    "                empty_count += 1\n",
    "        \n",
    "        if not processed_texts:\n",
    "            raise ValueError(\"No valid text data found after preprocessing\")\n",
    "        \n",
    "        # Store statistics\n",
    "        self.feature_stats_ = {\n",
    "            'total_samples': len(text_data),\n",
    "            'valid_samples': len(processed_texts),\n",
    "            'empty_samples': empty_count,\n",
    "            'avg_length': np.mean([len(text.split()) for text in processed_texts])\n",
    "        }\n",
    "        \n",
    "        self._log(f\"Preprocessing stats: {self.feature_stats_}\")\n",
    "        \n",
    "        # Initialize vectorizer\n",
    "        if self.use_tfidf:\n",
    "            self.vectorizer = TfidfVectorizer(**self.vectorizer_params)\n",
    "        else:\n",
    "            # Adjust parameters for CountVectorizer\n",
    "            count_params = self.vectorizer_params.copy()\n",
    "            if count_params.get('min_df', 0) < 1:\n",
    "                count_params['min_df'] = 1\n",
    "            self.vectorizer = CountVectorizer(**count_params)\n",
    "\n",
    "        self.vectorizer.fit(processed_texts)\n",
    "        self._log(\"Vectorizer fitted successfully\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform with better error handling.\"\"\"\n",
    "        if self.vectorizer is None:\n",
    "            raise ValueError(\"Preprocessor has not been fitted yet. Call fit() first.\")\n",
    "        \n",
    "        text_data = self._extract_text_from_input(X)\n",
    "        processed_texts = [\n",
    "            self.preprocess_text(self.clean_text(text)) \n",
    "            for text in text_data\n",
    "        ]\n",
    "        \n",
    "        return self.vectorizer.transform(processed_texts)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Get feature names with error handling.\"\"\"\n",
    "        if self.vectorizer is None:\n",
    "            raise ValueError(\"Preprocessor has not been fitted yet. Call fit() first\")\n",
    "        return self.vectorizer.get_feature_names_out()\n",
    "\n",
    "    def get_feature_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get preprocessing statistics.\"\"\"\n",
    "        return self.feature_stats_.copy()\n",
    "\n",
    "    def get_top_features(self, n=20) -> List[str]:\n",
    "        \"\"\"Get top n features by importance (for TF-IDF).\"\"\"\n",
    "        if self.vectorizer is None:\n",
    "            raise ValueError(\"Preprocessor has not been fitted yet\")\n",
    "        \n",
    "        if hasattr(self.vectorizer, 'idf_'):\n",
    "            # For TF-IDF, get features with highest IDF scores\n",
    "            feature_names = self.get_feature_names_out()\n",
    "            idf_scores = self.vectorizer.idf_\n",
    "            top_indices = np.argsort(idf_scores)[-n:][::-1]\n",
    "            return [feature_names[i] for i in top_indices]\n",
    "        else:\n",
    "            # For CountVectorizer, just return first n features\n",
    "            return list(self.get_feature_names_out()[:n])\n",
    "\n",
    "print(\"Enhanced TweetPreprocessor created with advanced features:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the whole class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_cleaning(tweets):\n",
    "    for i, tweet in enumerate(tweets[:3], start=1):\n",
    "        cleaned = preprocessor.preprocess_text(tweet)\n",
    "        print(f\"Tweet {i}:\\nOriginal: {tweet}\\nCleaned:  {cleaned}\\n\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:\n",
      "Original: Safaricom's network has been down all morning â€” can't even make a simple call. ðŸ™„ #Fail\n",
      "Cleaned:  safaricom network morning even make simple call fail\n",
      "\n",
      "Tweet 2:\n",
      "Original: Tried buying airtime on M-PESA and it vanished. No confirmation, no refund. Typical Safaricom. ðŸ˜¡\n",
      "Cleaned:  tried buying airtime pesa vanished confirmation refund typical safaricom\n",
      "\n",
      "Tweet 3:\n",
      "Original: Safaricom's data bundles expire faster than my morning coffee. Absolute scam. â˜•ðŸ“‰\n",
      "Cleaned:  safaricom data bundle expire faster morning coffee absolute scam\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = [\n",
    "    \"Safaricom's network has been down all morning â€” can't even make a simple call. ðŸ™„ #Fail\",\n",
    "    \"Tried buying airtime on M-PESA and it vanished. No confirmation, no refund. Typical Safaricom. ðŸ˜¡\",\n",
    "    \"Safaricom's data bundles expire faster than my morning coffee. Absolute scam. â˜•ðŸ“‰\"\n",
    "]\n",
    "test_text_cleaning(sample_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Testing fit and transform...\n",
      "\n",
      "Feature matrix shape: (3, 45)\n",
      "Feature matrix type: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Number of features: 45\n",
      "\n",
      "Sample features:\n",
      "absolute, absolute scam, airtime, airtime pesa, bundle, bundle expire, buying, buying airtime, call, call fail, coffee, coffee absolute, confirmation, confirmation refund, data, data bundle, even, even make, expire, expire faster\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: Testing fit and transform...\\n\")\n",
    "\n",
    "# Fit and transform\n",
    "preprocessor.fit(sample_tweets)\n",
    "feature_matrix = preprocessor.transform(sample_tweets)\n",
    "\n",
    "# Inspect the results\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"Feature matrix type: {type(feature_matrix)}\")\n",
    "print(f\"Number of features: {len(preprocessor.get_feature_names())}\\n\")\n",
    "\n",
    "# Preview feature names\n",
    "feature_names = preprocessor.get_feature_names()\n",
    "print(\"Sample features:\")\n",
    "print(\", \".join(feature_names[:20]))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
